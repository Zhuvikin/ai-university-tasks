{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center><img src='https://cdn-images-1.medium.com/max/2600/1*aqNgmfyBIStLrf9k7d9cng.jpeg'/></center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is PyTorch?\n",
    "\n",
    "PyTorch is a Python based computing library, targeted to two main goals:\n",
    "1. Replace Numpy to use the power of GPU\n",
    "2. Deeplaerning research platform which provides flexibility and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Current device is ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tensor</h2>\n",
    "\n",
    "Tensor is similar to Numpy ndarray, but provides copabilities of being calculated on GPU\n",
    "\n",
    "How can we create tensors in PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5418e+15,  4.5775e-41, -1.5418e+15,  4.5775e-41,  3.7415e-43,\n",
       "          0.0000e+00,  6.4890e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5918e-40,\n",
       "          2.6306e-22,  1.4013e-45],\n",
       "        [        nan,         nan,  0.0000e+00,  0.0000e+00,  2.6306e-22,\n",
       "          1.4013e-45, -1.5418e+15],\n",
       "        [ 4.5775e-41, -1.5418e+15,  4.5775e-41,  3.7695e-43,  0.0000e+00,\n",
       "          2.6278e-22,  1.4013e-45],\n",
       "        [-1.4214e+15,  4.5775e-41, -1.4214e+15,  4.5775e-41,  1.8021e-42,\n",
       "          0.0000e+00,  1.8063e-42]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating a new tensor of shape (5, 7)\n",
    "# Note: the values of each tensor element is trash\n",
    "torch.empty(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7767, 0.7629, 0.5792, 0.2430, 0.7086],\n",
       "        [0.0686, 0.1220, 0.1715, 0.5873, 0.6557],\n",
       "        [0.0746, 0.8218, 0.8368, 0.5573, 0.0762]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14,  4,  7, 15,  3],\n",
      "        [ 0,  1,  0, 22,  1],\n",
      "        [ 0,  1,  8, 16, 24],\n",
      "        [ 8, 19,  7, 18, 18],\n",
      "        [ 7, 12,  2, 12, 20]])\n",
      "Sliced  tensor([[ 1, 22],\n",
      "        [19, 18]])\n"
     ]
    }
   ],
   "source": [
    "# We can also use Numpy slicing to get the date from our tensors\n",
    "\n",
    "a = torch.randint(0, 25, (5,5))\n",
    "\n",
    "print(a)\n",
    "\n",
    "b = a[1::2, 1::2]\n",
    "\n",
    "print('Sliced ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Basic tensor operations</b>\n",
    "\n",
    "For exhaustive list look here: https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation  tensor([5, 7, 9])\n",
      "Multiplication  tensor([ 4, 10, 18])\n",
      "Dividing  tensor([4, 2, 2])\n",
      "Expanation  tensor([  1,  32, 729])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "\n",
    "print(\"Summation \", torch.add(a, b)) # the same as a + b\n",
    "\n",
    "print(\"Multiplication \", torch.mul(a, b)) # the same as a * b\n",
    "\n",
    "print(\"Dividing \", b / a)\n",
    "\n",
    "print(\"Expanation \", a ** b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tensors on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((5,5), device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Autograd: Automatic differentiation</h3>\n",
    "\n",
    "The main package for all of the NN in py torch is <b>autograde</b> package. This package provides automatic differentiation for all operations on Tensors. \n",
    "\n",
    "<b>torch.Tensor</b> is the main class of this package. If we want to start tracking all the operations on it we can simply set <b>requires_grad</b> prop as True and autograd start tracking all the operation on this tensor.\n",
    "\n",
    "If we want to stop tracking operations we can call <b>.detach</b> method of tensor and autograde stop tracking all the operations.\n",
    "\n",
    "We can also prevent autograd from tracking the history by wraping the code int <b> with torch.no_grad(): </b> block.\n",
    "\n",
    "In order to calculate the derivatives we can call <b>.backward()</b> method on a Tensor\n",
    "\n",
    "Let's take a look on an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((3,3), requires_grad=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 10., 10.],\n",
       "        [10., 10., 10.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a + 10\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1000., 1000., 1000.],\n",
       "        [1000., 1000., 1000.],\n",
       "        [1000., 1000., 1000.]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = b ** 3\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.sum(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[300., 300., 300.],\n",
       "        [300., 300., 300.],\n",
       "        [300., 300., 300.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "d = \\sum z_i, z_i =  (a + 10) ^ 3, z_i = 1000 \\\\\n",
    "Therefore, \\frac{\\partial d}{\\partial x} = 3 * (x_i + 10)^2 = 3 * (0 + 10)^2 = 300\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we understand how to create tensors in PyTorch and how the autgrad works we can try to build our first Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, input_dimension, hidden_dimension, output_dimension = 64, 16, 64, 10\n",
    "lr = 1e-4\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  torch.Size([64, 16])\n",
      "y size:  torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(batch_size, input_dimension, device=device)\n",
    "y = torch.randn(batch_size, output_dimension, device=device)\n",
    "\n",
    "print(\"x size: \", x.size())\n",
    "print(\"y size: \", y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(input_dimension, hidden_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dimension, output_dimension)            \n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 448.038330078125\n",
      "10 437.834228515625\n",
      "20 428.019775390625\n",
      "30 418.6061096191406\n",
      "40 409.5091552734375\n",
      "50 400.7294616699219\n",
      "60 392.2059326171875\n",
      "70 383.9371643066406\n",
      "80 375.8520202636719\n",
      "90 367.9934997558594\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    yhat = model(x)\n",
    "    \n",
    "    loss = loss_fn(yhat, y)\n",
    "    if i % 10 == 0:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backwprop.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Computer all the gradients of the loss in the respect to all learnable params\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using Gradient Decent\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we used Gradient Decent in order to update the weights of our model. This looks a little bit cranky and it wouldn't be really a good idea to write something like this all the time. Fortunatelly, PyTorch comes with a lot of useful classes which will helps us to use different strategies to update our weights.\n",
    "\n",
    "<b>torch.optim</b> packages comes with a lot of useful predefined classes and functions. For example:\n",
    "1. torch.optim.Adam\n",
    "2. torch.optim.SGD\n",
    "3. torch.optim.Adadelta\n",
    "4. torch.optim.Adagrad  \n",
    "\n",
    "For full list of optimizers take a look https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "So, now we can use torch.optim.* package to update our weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 360.3710632324219\n",
      "10 357.6697998046875\n",
      "20 355.0159912109375\n",
      "30 352.392822265625\n",
      "40 349.7917175292969\n",
      "50 347.2153015136719\n",
      "60 344.6536865234375\n",
      "70 342.1019592285156\n",
      "80 339.5645446777344\n",
      "90 337.0379333496094\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    yhat = model(x)\n",
    "    \n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    # Zero the gradients before running the backwprop.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Computer all the gradients of the loss in the respect to all learnable params\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using Gradient Decent\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i, loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "1. Generate random dataset which has batch_size = 128 and 15 input features\n",
    "2. Generate random result labels with size 128 and 5\n",
    "3. Create a simple NN with next architecture Linear -> Relu -> Linear -> Relu -> Linear. \n",
    "    The first hidden layer has 64 neurons, second hidden layer has 32 neurons\n",
    "4. use SGD optimizer, for updating the weights\n",
    "5. Train your network on 500 epochs, print the loss each 100 epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x size:  torch.Size([128, 15])\n",
      "y size:  torch.Size([128, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size, input_dimension, output_dimension = 128, 15, 5\n",
    "\n",
    "hidden1_dimension, hidden2_dimension = 64, 32\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "x = torch.randn(batch_size, input_dimension, device=device)\n",
    "y = torch.randn(batch_size, output_dimension, device=device)\n",
    "\n",
    "print(\"x size: \", x.size())\n",
    "print(\"y size: \", y.size())\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(input_dimension, hidden1_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_dimension, hidden2_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_dimension, output_dimension),\n",
    "        ).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 640.3190307617188\n",
      "10 629.2278442382812\n",
      "20 621.572998046875\n",
      "30 615.1554565429688\n",
      "40 609.4373168945312\n",
      "50 603.9766845703125\n",
      "60 598.6102905273438\n",
      "70 593.2940673828125\n",
      "80 587.8413696289062\n",
      "90 582.2498168945312\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    yhat = model(x)\n",
    "    \n",
    "    loss = loss_fn(yhat, y)\n",
    "    if i % 10 == 0:\n",
    "        print(i, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backwprop.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Computer all the gradients of the loss in the respect to all learnable params\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using Gradient Decent\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data -= lr * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 576.5977172851562\n",
      "10 573.703857421875\n",
      "20 570.8494873046875\n",
      "30 568.000732421875\n",
      "40 565.1248779296875\n",
      "50 562.21337890625\n",
      "60 559.2603149414062\n",
      "70 556.2955932617188\n",
      "80 553.31640625\n",
      "90 550.3402709960938\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(epochs):\n",
    "    yhat = model(x)\n",
    "    \n",
    "    loss = loss_fn(yhat, y)\n",
    "\n",
    "    # Zero the gradients before running the backwprop.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Computer all the gradients of the loss in the respect to all learnable params\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using Gradient Decent\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(i, loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we used random-generated datasets. Let's take a look how we can load datasets in PyTorch. Pytorch comes with a lot of predefined datasets which we can play with.  \n",
    "Let's for example load MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets.MNIST('data/', \n",
    "                           download=True, \n",
    "                           train=True, \n",
    "                           transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test_set = datasets.MNIST('data/', \n",
    "                          download=True, \n",
    "                          train=False, \n",
    "                          transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data size: ', train_set.train_data.size())\n",
    "print('Train data labels: ', train_set.train_labels.size())\n",
    "\n",
    "print('Test data size: ', test_set.test_data.size())\n",
    "print('Test data labels: ', test_set.test_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can feed our train and test set to the DataLoader which will compact our features in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, device, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch, (target, labels) in enumerate(train_loader):\n",
    "            target = target.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the gradients before running the backwprop.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            yhat = model(target)\n",
    "\n",
    "            loss = criterion(yhat, labels)\n",
    "\n",
    "            # Computer all the gradients of the loss in the respect to all learnable params\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weight\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch % 2000 == 1999:\n",
    "                print('Epoch {}, Train loss {}'.format(e, total_loss / 2000))\n",
    "                total_loss = 0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(NeuralNetwork().to(device), train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataLoader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for target, labels in dataLoader:\n",
    "            target, labels = target.to(device), labels.to(device)\n",
    "            yhat = model(target)\n",
    "            test_loss += F.nll_loss(yhat, labels, reduction='sum').item()\n",
    "            prediction = yhat.argmax(dim=1, keepdim=True)\n",
    "            correct += prediction.eq(labels.view_as(prediction)).sum().item()\n",
    "\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(dataLoader.dataset), 100. * correct / len(dataLoader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we have learnt how to create, train, and test our simple Neural Network. We also got very high accuracy 96%, it's not the state of the art though. So, if 96% is not good, what can we do?\n",
    "There is a better way to solve computer vision tasks than simple shallow NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Convolutional Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*uAeANQIOQPqWZnnuH-VEyw.jpeg\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is deeplearning algorithm which can take an image as input, assign weights (importance) to different aspect of the image and be able to differentiate one from another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we understood CNN can include many deferent layers such as Convolutional layer, max pooling layer, and others. Let's try to build one and see if our new model can bit the simple one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Cnn().to(device)\n",
    "\n",
    "cnn = train_model(cnn, train_loader, device)\n",
    "\n",
    "test_model(cnn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "1. Load FashionMnist dataset, e.g datasets.FashionMNIST -> train_fashion_set and test_fashion_set\n",
    "2. Create two data loaders for train and test sets, e.g. train_fashion_loader, test_fashion_loader\n",
    "3. Create a simple NN with any architecture that you like any numbers of layers and neurons\n",
    "4. Train the network network on train_fashion_mnist data set\n",
    "5. Test the network on test_fashion_mnist data set\n",
    "6. Report the total accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "1. Create CNN with next architecture  \n",
    "    -> Convolution layer ( 1 - input features, 6 - output features, kernel size - (5,5), stride - 1, padding - 2  \n",
    "    -> Max pooling layer ( 2 - kernel size)  \n",
    "    -> Convolution layer ( 6 - input features, 16 - output features, kernel size - (5,5), stride - 1, padding - 0  \n",
    "    -> Max pooling layer ( 2 - kernel size)  \n",
    "    -> Dence layer (5 * 5 * 16 - input features, 120 - output features)  \n",
    "    -> Dence layer (120 - input features, 84 - output features)  \n",
    "    -> Dence layer (84 - input features, 10 - output features)  \n",
    "    -> Use Relu as activation function  \n",
    "    \n",
    "    \n",
    "2. Use CrossEntropyLoss function\n",
    "3. Use SGD optimizer with learning rate - 0.001, momentum - 0.9\n",
    "4. Train network with 10 epochs\n",
    "5. Test network on test_fashion_set dataset "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JKERNEL",
   "language": "python",
   "name": "jkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
